{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download() # one time\n",
    "#nltk.download('punkt') # first-time use only\n",
    "#nltk.download('wordnet') # first-time use only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import string # to process standard python strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f=open('Sample Corpus - GL Bot.json','r',errors = 'ignore')\n",
    "#raw=f.read()\n",
    "#raw=raw.lower()# converts to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy\n",
    "import random\n",
    "import json\n",
    "f=open('Sample Corpus - GL Bot.json','r',errors = 'ignore')\n",
    "#with open(Sample Corpus - GL Bot.json') as file:\n",
    "#    data = json.load(file)\n",
    "\n",
    "with f as file :\n",
    "    data = json.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "        \n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'abl',\n",
       " 'access',\n",
       " 'act',\n",
       " 'ad',\n",
       " 'adam',\n",
       " 'aifl',\n",
       " 'aiml',\n",
       " 'am',\n",
       " 'an',\n",
       " 'anyon',\n",
       " 'ar',\n",
       " 'art',\n",
       " 'backward',\n",
       " 'bad',\n",
       " 'bag',\n",
       " 'batch',\n",
       " 'bay',\n",
       " 'belong',\n",
       " 'best',\n",
       " 'blend',\n",
       " 'bloody',\n",
       " 'boost',\n",
       " 'bot',\n",
       " 'buddy',\n",
       " 'class',\n",
       " 'contact',\n",
       " 'cre',\n",
       " 'cross',\n",
       " 'cya',\n",
       " 'day',\n",
       " 'deep',\n",
       " 'did',\n",
       " 'diffult',\n",
       " 'do',\n",
       " 'ensembl',\n",
       " 'epoch',\n",
       " 'explain',\n",
       " 'first',\n",
       " 'for',\n",
       " 'forest',\n",
       " 'forward',\n",
       " 'from',\n",
       " 'funct',\n",
       " 'good',\n",
       " 'goodby',\n",
       " 'grady',\n",
       " 'gre',\n",
       " 'hat',\n",
       " 'hav',\n",
       " 'hel',\n",
       " 'hello',\n",
       " 'help',\n",
       " 'hey',\n",
       " 'hi',\n",
       " 'hid',\n",
       " 'hour',\n",
       " 'how',\n",
       " 'hyp',\n",
       " 'i',\n",
       " 'imput',\n",
       " 'in',\n",
       " 'intellig',\n",
       " 'is',\n",
       " 'jerk',\n",
       " 'jok',\n",
       " 'knn',\n",
       " 'lat',\n",
       " 'lay',\n",
       " 'learn',\n",
       " 'leav',\n",
       " 'link',\n",
       " 'list',\n",
       " 'log',\n",
       " 'lot',\n",
       " 'machin',\n",
       " 'me',\n",
       " 'ml',\n",
       " 'my',\n",
       " 'naiv',\n",
       " 'nam',\n",
       " 'nb',\n",
       " 'net',\n",
       " 'network',\n",
       " 'neur',\n",
       " 'no',\n",
       " 'not',\n",
       " 'of',\n",
       " 'olymp',\n",
       " 'olyp',\n",
       " 'on',\n",
       " 'onlin',\n",
       " 'op',\n",
       " 'opert',\n",
       " 'otim',\n",
       " 'paramet',\n",
       " 'piec',\n",
       " 'plea',\n",
       " 'pm',\n",
       " 'problem',\n",
       " 'prop',\n",
       " 'random',\n",
       " 'regress',\n",
       " 'relu',\n",
       " 'screw',\n",
       " 'see',\n",
       " 'sgd',\n",
       " 'shit',\n",
       " 'sigmoid',\n",
       " 'sl',\n",
       " 'smart',\n",
       " 'softmax',\n",
       " 'solv',\n",
       " 'stupid',\n",
       " 'superv',\n",
       " 'svm',\n",
       " 'talk',\n",
       " 'teach',\n",
       " 'techb=niques',\n",
       " 'techn',\n",
       " 'thank',\n",
       " 'the',\n",
       " 'ther',\n",
       " 'think',\n",
       " 'ticket',\n",
       " 'tim',\n",
       " 'to',\n",
       " 'ton',\n",
       " 'too',\n",
       " 'tool',\n",
       " 'un',\n",
       " 'understand',\n",
       " 'up',\n",
       " 'us',\n",
       " 'useless',\n",
       " 'valid',\n",
       " 'very',\n",
       " 'vis',\n",
       " 'wast',\n",
       " 'weight',\n",
       " 'what',\n",
       " 'when',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'window',\n",
       " 'with',\n",
       " 'work',\n",
       " 'ya',\n",
       " 'yo',\n",
       " 'you']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "words = sorted(list(set(words)))\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hi'],\n",
       " ['how', 'are', 'you'],\n",
       " ['is', 'anyone', 'there'],\n",
       " ['hello'],\n",
       " ['whats', 'up'],\n",
       " ['hey'],\n",
       " ['yo'],\n",
       " ['listen'],\n",
       " ['please', 'help', 'me'],\n",
       " ['i', 'am', 'learner', 'from'],\n",
       " ['i', 'belong', 'to'],\n",
       " ['aiml', 'batch'],\n",
       " ['aifl', 'batch'],\n",
       " ['i', 'am', 'from'],\n",
       " ['my', 'pm', 'is'],\n",
       " ['blended'],\n",
       " ['online'],\n",
       " ['i', 'am', 'from'],\n",
       " ['hey', 'ya'],\n",
       " ['talking', 'to', 'you', 'for', 'first', 'time'],\n",
       " ['thank', 'you'],\n",
       " ['thanks'],\n",
       " ['cya'],\n",
       " ['see', 'you'],\n",
       " ['later'],\n",
       " ['see', 'you', 'later'],\n",
       " ['goodbye'],\n",
       " ['i', 'am', 'leaving'],\n",
       " ['have', 'a', 'Good', 'day'],\n",
       " ['you', 'helped', 'me'],\n",
       " ['thanks', 'a', 'lot'],\n",
       " ['thanks', 'a', 'ton'],\n",
       " ['you', 'are', 'the', 'best'],\n",
       " ['great', 'help'],\n",
       " ['too', 'good'],\n",
       " ['you', 'are', 'a', 'good', 'learning', 'buddy'],\n",
       " ['olympus'],\n",
       " ['explain', 'me', 'how', 'olympus', 'works'],\n",
       " ['I', 'am', 'not', 'able', 'to', 'understand', 'olympus'],\n",
       " ['olympus', 'window', 'not', 'working'],\n",
       " ['no', 'access', 'to', 'olympus'],\n",
       " ['unable', 'to', 'see', 'link', 'in', 'olympus'],\n",
       " ['no', 'link', 'visible', 'on', 'olympus'],\n",
       " ['whom', 'to', 'contact', 'for', 'olympus'],\n",
       " ['lot', 'of', 'problem', 'with', 'olympus'],\n",
       " ['olypus', 'is', 'not', 'a', 'good', 'tool'],\n",
       " ['lot', 'of', 'problems', 'with', 'olympus'],\n",
       " ['how', 'to', 'use', 'olympus'],\n",
       " ['teach', 'me', 'olympus'],\n",
       " ['i', 'am', 'not', 'able', 'to', 'understand', 'svm'],\n",
       " ['explain', 'me', 'how', 'machine', 'learning', 'works'],\n",
       " ['i', 'am', 'not', 'able', 'to', 'understand', 'naive', 'bayes'],\n",
       " ['i', 'am', 'not', 'able', 'to', 'understand', 'logistic', 'regression'],\n",
       " ['i', 'am', 'not', 'able', 'to', 'understand', 'ensemble', 'techb=niques'],\n",
       " ['i', 'am', 'not', 'able', 'to', 'understand', 'knn'],\n",
       " ['i', 'am', 'not', 'able', 'to', 'understand', 'knn', 'imputer'],\n",
       " ['i', 'am', 'not', 'able', 'to', 'understand', 'cross', 'validation'],\n",
       " ['i', 'am', 'not', 'able', 'to', 'understand', 'boosting'],\n",
       " ['i', 'am', 'not', 'able', 'to', 'understand', 'random', 'forest'],\n",
       " ['i', 'am', 'not', 'able', 'to', 'understand', 'ada', 'boosting'],\n",
       " ['i', 'am', 'not', 'able', 'to', 'understand', 'gradient', 'boosting'],\n",
       " ['machine', 'learning'],\n",
       " ['ML'],\n",
       " ['SL'],\n",
       " ['supervised', 'learning'],\n",
       " ['knn'],\n",
       " ['logistic', 'regression'],\n",
       " ['regression'],\n",
       " ['classification'],\n",
       " ['naive', 'bayes'],\n",
       " ['nb'],\n",
       " ['ensemble', 'techniques'],\n",
       " ['bagging'],\n",
       " ['boosting'],\n",
       " ['ada', 'boosting'],\n",
       " ['ada'],\n",
       " ['gradient', 'boosting'],\n",
       " ['hyper', 'parameters'],\n",
       " ['what', 'is', 'deep', 'learning'],\n",
       " ['unable', 'to', 'understand', 'deep', 'learning'],\n",
       " ['explain', 'me', 'how', 'deep', 'learning', 'works'],\n",
       " ['i', 'am', 'not', 'able', 'to', 'understand', 'deep', 'learning'],\n",
       " ['not', 'able', 'to', 'understand', 'neural', 'nets'],\n",
       " ['very', 'diffult', 'to', 'understand', 'neural', 'nets'],\n",
       " ['unable', 'to', 'understand', 'neural', 'nets'],\n",
       " ['ann'],\n",
       " ['artificial', 'intelligence'],\n",
       " ['artificial', 'neural', 'networks'],\n",
       " ['weights'],\n",
       " ['activation', 'function'],\n",
       " ['hidden', 'layers'],\n",
       " ['softmax'],\n",
       " ['sigmoid'],\n",
       " ['relu'],\n",
       " ['otimizer'],\n",
       " ['forward', 'propagation'],\n",
       " ['backward', 'propagation'],\n",
       " ['epochs'],\n",
       " ['epoch'],\n",
       " ['what', 'is', 'an', 'epoch'],\n",
       " ['adam'],\n",
       " ['sgd'],\n",
       " ['what', 'is', 'your', 'name'],\n",
       " ['who', 'are', 'you'],\n",
       " ['name', 'please'],\n",
       " ['when', 'are', 'your', 'hours', 'of', 'opertions'],\n",
       " ['what', 'are', 'your', 'working', 'hours'],\n",
       " ['hours', 'of', 'operation'],\n",
       " ['working', 'hours'],\n",
       " ['hours'],\n",
       " ['what', 'the', 'hell'],\n",
       " ['bloody', 'stupid', 'bot'],\n",
       " ['do', 'you', 'think', 'you', 'are', 'very', 'smart'],\n",
       " ['screw', 'you'],\n",
       " ['i', 'hate', 'you'],\n",
       " ['you', 'are', 'stupid'],\n",
       " ['jerk'],\n",
       " ['you', 'are', 'a', 'joke'],\n",
       " ['useless', 'piece', 'of', 'shit'],\n",
       " ['my', 'problem', 'is', 'not', 'solved'],\n",
       " ['you', 'did', 'not', 'help', 'me'],\n",
       " ['not', 'a', 'good', 'solution'],\n",
       " ['bad', 'solution'],\n",
       " ['not', 'good', 'solution'],\n",
       " ['no', 'help'],\n",
       " ['wasted', 'my', 'time'],\n",
       " ['useless', 'bot'],\n",
       " ['create', 'a', 'ticket']]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Intro',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Exit',\n",
       " 'Olympus',\n",
       " 'Olympus',\n",
       " 'Olympus',\n",
       " 'Olympus',\n",
       " 'Olympus',\n",
       " 'Olympus',\n",
       " 'Olympus',\n",
       " 'Olympus',\n",
       " 'Olympus',\n",
       " 'Olympus',\n",
       " 'Olympus',\n",
       " 'Olympus',\n",
       " 'Olympus',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'SL',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'NN',\n",
       " 'Bot',\n",
       " 'Bot',\n",
       " 'Bot',\n",
       " 'Bot',\n",
       " 'Bot',\n",
       " 'Bot',\n",
       " 'Bot',\n",
       " 'Bot',\n",
       " 'Profane',\n",
       " 'Profane',\n",
       " 'Profane',\n",
       " 'Profane',\n",
       " 'Profane',\n",
       " 'Profane',\n",
       " 'Profane',\n",
       " 'Profane',\n",
       " 'Profane',\n",
       " 'Ticket',\n",
       " 'Ticket',\n",
       " 'Ticket',\n",
       " 'Ticket',\n",
       " 'Ticket',\n",
       " 'Ticket',\n",
       " 'Ticket',\n",
       " 'Ticket',\n",
       " 'Ticket']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bot', 'Exit', 'Intro', 'NN', 'Olympus', 'Profane', 'SL', 'Ticket']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sorted(labels)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "out_empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "\n",
    "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "\n",
    "\n",
    "training = numpy.array(training)\n",
    "output = numpy.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-672af089ec67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msent_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# converts to list of sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mword_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# converts to list of words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m    105\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1275\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m         \"\"\"\n\u001b[1;32m-> 1277\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \"\"\"\n\u001b[1;32m-> 1331\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \"\"\"\n\u001b[1;32m-> 1331\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \"\"\"\n\u001b[0;32m   1361\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1362\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'after_tok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "sent_tokens = nltk.sent_tokenize(words)# converts to list of sentences \n",
    "word_tokens = nltk.word_tokenize(words)# converts to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response=''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx=vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf==0):\n",
    "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
    "        return robo_response\n",
    "    else:\n",
    "        robo_response = robo_response+sent_tokens[idx]\n",
    "        return robo_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
      "Hi\n",
      "ROBO: hi\n",
      "Olympus\n",
      "ROBO: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        {\"tag\": \"exit\",\n",
      "         \"patterns\": [\"thank you\", \n",
      "                      \"thanks\", \n",
      "                      \"cya\",\n",
      "                      \"see you\",\n",
      "                      \"later\", \n",
      "                      \"see you later\", \n",
      "                      \"goodbye\", \n",
      "                      \"i am leaving\", \n",
      "                      \"have a good day\",\n",
      "                      \"you helped me\",\n",
      "                      \"thanks a lot\",\n",
      "                      \"thanks a ton\",\n",
      "                      \"you are the best\",\n",
      "                      \"great help\",\n",
      "                      \"too good\",\n",
      "                      \"you are a good learning buddy\"],\n",
      "         \"responses\": [\"i hope i was able to assist you, good bye\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        {\"tag\": \"olympus\",\n",
      "         \"patterns\": [\"olympus\",\n",
      "                      \"explain me how olympus works\",\n",
      "                      \"i am not able to understand olympus\",\n",
      "                      \"olympus window not working\",\n",
      "                      \"no access to olympus\",\n",
      "                      \"unable to see link in olympus\",\n",
      "                      \"no link visible on olympus\",\n",
      "                      \"whom to contact for olympus\",\n",
      "                      \"lot of problem with olympus\",\n",
      "                      \"olypus is not a good tool\",\n",
      "                      \"lot of problems with olympus\",\n",
      "                      \"how to use olympus\",\n",
      "                      \"teach me olympus\"],\n",
      "         \"responses\": [\"link: olympus wiki\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        {\"tag\": \"sl\",\n",
      "         \"patterns\": [\"i am not able to understand svm\", \n",
      "                      \"explain me how machine learning works\",\n",
      "                      \"i am not able to understand naive bayes\",\n",
      "                      \"i am not able to understand logistic regression\",\n",
      "                      \"i am not able to understand ensemble techb=niques\",\n",
      "                      \"i am not able to understand knn\",\n",
      "                      \"i am not able to understand knn imputer\",\n",
      "                      \"i am not able to understand cross validation\",\n",
      "                      \"i am not able to understand boosting\",\n",
      "                      \"i am not able to understand random forest\",\n",
      "                      \"i am not able to understand ada boosting\",\n",
      "                      \"i am not able to understand gradient boosting\",\n",
      "                      \"machine learning\",\n",
      "                      \"ml\",\n",
      "                      \"sl\", \n",
      "                      \"supervised learning\",\n",
      "                      \"knn\",\n",
      "                      \"logistic regression\",\n",
      "                      \"regression\",\n",
      "                      \"classification\",\n",
      "                      \"naive bayes\",\n",
      "                      \"nb\",\n",
      "                      \"ensemble techniques\",\n",
      "                      \"bagging\",\n",
      "                      \"boosting\",\n",
      "                      \"ada boosting\",\n",
      "                      \"ada\",\n",
      "                      \"gradient boosting\",\n",
      "                      \"hyper parameters\"],\n",
      "         \"responses\": [\"link: machine learning wiki \"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "    \n",
      "        {\"tag\": \"nn\",\n",
      "         \"patterns\": [\"what is deep learning\", \n",
      "                      \"unable to understand deep learning\",\n",
      "                      \"explain me how deep learning works\",\n",
      "                      \"i am not able to understand deep learning\",\n",
      "                      \"not able to understand neural nets\",\n",
      "                      \"very diffult to understand neural nets\",\n",
      "                      \"unable to understand neural nets\",\n",
      "                      \"ann\", \n",
      "                      \"artificial intelligence\", \n",
      "                      \"artificial neural networks\",\n",
      "                      \"weights\",\n",
      "                      \"activation function\",\n",
      "                      \"hidden layers\",\n",
      "                      \"softmax\",\n",
      "                      \"sigmoid\",\n",
      "                      \"relu\",\n",
      "                      \"otimizer\",\n",
      "                      \"forward propagation\",\n",
      "                      \"backward propagation\",\n",
      "                      \"epochs\",\n",
      "                      \"epoch\",\n",
      "                      \"what is an epoch\",\n",
      "                      \"adam\",\n",
      "                      \"sgd\"],\n",
      "         \"responses\": [\"link: neural nets wiki\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        \n",
      "        {\"tag\": \"bot\",\n",
      "         \"patterns\": [\"what is your name\",\n",
      "                      \"who are you\",\n",
      "                      \"name please\",\n",
      "                      \"when are your hours of opertions\", \n",
      "                      \"what are your working hours\", \n",
      "                      \"hours of operation\",\n",
      "                      \"working hours\",\n",
      "                      \"hours\"],\n",
      "         \"responses\": [\"i am your virtual learning assistant\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "    \n",
      "    {\"tag\": \"profane\",\n",
      "         \"patterns\": [\"what the hell\",\n",
      "                      \"bloody stupid bot\",\n",
      "                      \"do you think you are very smart\",\n",
      "                      \"screw you\", \n",
      "                      \"i hate you\", \n",
      "                      \"you are stupid\",\n",
      "                      \"jerk\",\n",
      "                      \"you are a joke\",\n",
      "                      \"useless piece of shit\"],\n",
      "         \"responses\": [\"please use respectful words\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "    \n",
      "        {\"tag\": \"ticket\",\n",
      "         \"patterns\": [\"my problem is not solved\", \n",
      "                      \"you did not help me\", \n",
      "                      \"not a good solution\",\n",
      "                      \"bad solution\",\n",
      "                      \"not good solution\",\n",
      "                      \"no help\",\n",
      "                      \"wasted my time\",\n",
      "                      \"useless bot\",\n",
      "                      \"create a ticket\"],\n",
      "         \"responses\": [\"tarnsferring the request to your pm\"],\n",
      "         \"context_set\": \"\"\n",
      "        }\n",
      "   ]\n",
      "}\n",
      "explain me how olympus works\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: \"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        {\"tag\": \"exit\",\n",
      "         \"patterns\": [\"thank you\", \n",
      "                      \"thanks\", \n",
      "                      \"cya\",\n",
      "                      \"see you\",\n",
      "                      \"later\", \n",
      "                      \"see you later\", \n",
      "                      \"goodbye\", \n",
      "                      \"i am leaving\", \n",
      "                      \"have a good day\",\n",
      "                      \"you helped me\",\n",
      "                      \"thanks a lot\",\n",
      "                      \"thanks a ton\",\n",
      "                      \"you are the best\",\n",
      "                      \"great help\",\n",
      "                      \"too good\",\n",
      "                      \"you are a good learning buddy\"],\n",
      "         \"responses\": [\"i hope i was able to assist you, good bye\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        {\"tag\": \"olympus\",\n",
      "         \"patterns\": [\"olympus\",\n",
      "                      \"explain me how olympus works\",\n",
      "                      \"i am not able to understand olympus\",\n",
      "                      \"olympus window not working\",\n",
      "                      \"no access to olympus\",\n",
      "                      \"unable to see link in olympus\",\n",
      "                      \"no link visible on olympus\",\n",
      "                      \"whom to contact for olympus\",\n",
      "                      \"lot of problem with olympus\",\n",
      "                      \"olypus is not a good tool\",\n",
      "                      \"lot of problems with olympus\",\n",
      "                      \"how to use olympus\",\n",
      "                      \"teach me olympus\"],\n",
      "         \"responses\": [\"link: olympus wiki\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        {\"tag\": \"sl\",\n",
      "         \"patterns\": [\"i am not able to understand svm\", \n",
      "                      \"explain me how machine learning works\",\n",
      "                      \"i am not able to understand naive bayes\",\n",
      "                      \"i am not able to understand logistic regression\",\n",
      "                      \"i am not able to understand ensemble techb=niques\",\n",
      "                      \"i am not able to understand knn\",\n",
      "                      \"i am not able to understand knn imputer\",\n",
      "                      \"i am not able to understand cross validation\",\n",
      "                      \"i am not able to understand boosting\",\n",
      "                      \"i am not able to understand random forest\",\n",
      "                      \"i am not able to understand ada boosting\",\n",
      "                      \"i am not able to understand gradient boosting\",\n",
      "                      \"machine learning\",\n",
      "                      \"ml\",\n",
      "                      \"sl\", \n",
      "                      \"supervised learning\",\n",
      "                      \"knn\",\n",
      "                      \"logistic regression\",\n",
      "                      \"regression\",\n",
      "                      \"classification\",\n",
      "                      \"naive bayes\",\n",
      "                      \"nb\",\n",
      "                      \"ensemble techniques\",\n",
      "                      \"bagging\",\n",
      "                      \"boosting\",\n",
      "                      \"ada boosting\",\n",
      "                      \"ada\",\n",
      "                      \"gradient boosting\",\n",
      "                      \"hyper parameters\"],\n",
      "         \"responses\": [\"link: machine learning wiki \"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "    \n",
      "        {\"tag\": \"nn\",\n",
      "         \"patterns\": [\"what is deep learning\", \n",
      "                      \"unable to understand deep learning\",\n",
      "                      \"explain me how deep learning works\",\n",
      "                      \"i am not able to understand deep learning\",\n",
      "                      \"not able to understand neural nets\",\n",
      "                      \"very diffult to understand neural nets\",\n",
      "                      \"unable to understand neural nets\",\n",
      "                      \"ann\", \n",
      "                      \"artificial intelligence\", \n",
      "                      \"artificial neural networks\",\n",
      "                      \"weights\",\n",
      "                      \"activation function\",\n",
      "                      \"hidden layers\",\n",
      "                      \"softmax\",\n",
      "                      \"sigmoid\",\n",
      "                      \"relu\",\n",
      "                      \"otimizer\",\n",
      "                      \"forward propagation\",\n",
      "                      \"backward propagation\",\n",
      "                      \"epochs\",\n",
      "                      \"epoch\",\n",
      "                      \"what is an epoch\",\n",
      "                      \"adam\",\n",
      "                      \"sgd\"],\n",
      "         \"responses\": [\"link: neural nets wiki\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        \n",
      "        {\"tag\": \"bot\",\n",
      "         \"patterns\": [\"what is your name\",\n",
      "                      \"who are you\",\n",
      "                      \"name please\",\n",
      "                      \"when are your hours of opertions\", \n",
      "                      \"what are your working hours\", \n",
      "                      \"hours of operation\",\n",
      "                      \"working hours\",\n",
      "                      \"hours\"],\n",
      "         \"responses\": [\"i am your virtual learning assistant\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "    \n",
      "    {\"tag\": \"profane\",\n",
      "         \"patterns\": [\"what the hell\",\n",
      "                      \"bloody stupid bot\",\n",
      "                      \"do you think you are very smart\",\n",
      "                      \"screw you\", \n",
      "                      \"i hate you\", \n",
      "                      \"you are stupid\",\n",
      "                      \"jerk\",\n",
      "                      \"you are a joke\",\n",
      "                      \"useless piece of shit\"],\n",
      "         \"responses\": [\"please use respectful words\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "    \n",
      "        {\"tag\": \"ticket\",\n",
      "         \"patterns\": [\"my problem is not solved\", \n",
      "                      \"you did not help me\", \n",
      "                      \"not a good solution\",\n",
      "                      \"bad solution\",\n",
      "                      \"not good solution\",\n",
      "                      \"no help\",\n",
      "                      \"wasted my time\",\n",
      "                      \"useless bot\",\n",
      "                      \"create a ticket\"],\n",
      "         \"responses\": [\"tarnsferring the request to your pm\"],\n",
      "         \"context_set\": \"\"\n",
      "        }\n",
      "   ]\n",
      "}\n",
      "supervised learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: \"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        {\"tag\": \"exit\",\n",
      "         \"patterns\": [\"thank you\", \n",
      "                      \"thanks\", \n",
      "                      \"cya\",\n",
      "                      \"see you\",\n",
      "                      \"later\", \n",
      "                      \"see you later\", \n",
      "                      \"goodbye\", \n",
      "                      \"i am leaving\", \n",
      "                      \"have a good day\",\n",
      "                      \"you helped me\",\n",
      "                      \"thanks a lot\",\n",
      "                      \"thanks a ton\",\n",
      "                      \"you are the best\",\n",
      "                      \"great help\",\n",
      "                      \"too good\",\n",
      "                      \"you are a good learning buddy\"],\n",
      "         \"responses\": [\"i hope i was able to assist you, good bye\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        {\"tag\": \"olympus\",\n",
      "         \"patterns\": [\"olympus\",\n",
      "                      \"explain me how olympus works\",\n",
      "                      \"i am not able to understand olympus\",\n",
      "                      \"olympus window not working\",\n",
      "                      \"no access to olympus\",\n",
      "                      \"unable to see link in olympus\",\n",
      "                      \"no link visible on olympus\",\n",
      "                      \"whom to contact for olympus\",\n",
      "                      \"lot of problem with olympus\",\n",
      "                      \"olypus is not a good tool\",\n",
      "                      \"lot of problems with olympus\",\n",
      "                      \"how to use olympus\",\n",
      "                      \"teach me olympus\"],\n",
      "         \"responses\": [\"link: olympus wiki\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        {\"tag\": \"sl\",\n",
      "         \"patterns\": [\"i am not able to understand svm\", \n",
      "                      \"explain me how machine learning works\",\n",
      "                      \"i am not able to understand naive bayes\",\n",
      "                      \"i am not able to understand logistic regression\",\n",
      "                      \"i am not able to understand ensemble techb=niques\",\n",
      "                      \"i am not able to understand knn\",\n",
      "                      \"i am not able to understand knn imputer\",\n",
      "                      \"i am not able to understand cross validation\",\n",
      "                      \"i am not able to understand boosting\",\n",
      "                      \"i am not able to understand random forest\",\n",
      "                      \"i am not able to understand ada boosting\",\n",
      "                      \"i am not able to understand gradient boosting\",\n",
      "                      \"machine learning\",\n",
      "                      \"ml\",\n",
      "                      \"sl\", \n",
      "                      \"supervised learning\",\n",
      "                      \"knn\",\n",
      "                      \"logistic regression\",\n",
      "                      \"regression\",\n",
      "                      \"classification\",\n",
      "                      \"naive bayes\",\n",
      "                      \"nb\",\n",
      "                      \"ensemble techniques\",\n",
      "                      \"bagging\",\n",
      "                      \"boosting\",\n",
      "                      \"ada boosting\",\n",
      "                      \"ada\",\n",
      "                      \"gradient boosting\",\n",
      "                      \"hyper parameters\"],\n",
      "         \"responses\": [\"link: machine learning wiki \"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "    \n",
      "        {\"tag\": \"nn\",\n",
      "         \"patterns\": [\"what is deep learning\", \n",
      "                      \"unable to understand deep learning\",\n",
      "                      \"explain me how deep learning works\",\n",
      "                      \"i am not able to understand deep learning\",\n",
      "                      \"not able to understand neural nets\",\n",
      "                      \"very diffult to understand neural nets\",\n",
      "                      \"unable to understand neural nets\",\n",
      "                      \"ann\", \n",
      "                      \"artificial intelligence\", \n",
      "                      \"artificial neural networks\",\n",
      "                      \"weights\",\n",
      "                      \"activation function\",\n",
      "                      \"hidden layers\",\n",
      "                      \"softmax\",\n",
      "                      \"sigmoid\",\n",
      "                      \"relu\",\n",
      "                      \"otimizer\",\n",
      "                      \"forward propagation\",\n",
      "                      \"backward propagation\",\n",
      "                      \"epochs\",\n",
      "                      \"epoch\",\n",
      "                      \"what is an epoch\",\n",
      "                      \"adam\",\n",
      "                      \"sgd\"],\n",
      "         \"responses\": [\"link: neural nets wiki\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        \n",
      "        {\"tag\": \"bot\",\n",
      "         \"patterns\": [\"what is your name\",\n",
      "                      \"who are you\",\n",
      "                      \"name please\",\n",
      "                      \"when are your hours of opertions\", \n",
      "                      \"what are your working hours\", \n",
      "                      \"hours of operation\",\n",
      "                      \"working hours\",\n",
      "                      \"hours\"],\n",
      "         \"responses\": [\"i am your virtual learning assistant\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "    \n",
      "    {\"tag\": \"profane\",\n",
      "         \"patterns\": [\"what the hell\",\n",
      "                      \"bloody stupid bot\",\n",
      "                      \"do you think you are very smart\",\n",
      "                      \"screw you\", \n",
      "                      \"i hate you\", \n",
      "                      \"you are stupid\",\n",
      "                      \"jerk\",\n",
      "                      \"you are a joke\",\n",
      "                      \"useless piece of shit\"],\n",
      "         \"responses\": [\"please use respectful words\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "    \n",
      "        {\"tag\": \"ticket\",\n",
      "         \"patterns\": [\"my problem is not solved\", \n",
      "                      \"you did not help me\", \n",
      "                      \"not a good solution\",\n",
      "                      \"bad solution\",\n",
      "                      \"not good solution\",\n",
      "                      \"no help\",\n",
      "                      \"wasted my time\",\n",
      "                      \"useless bot\",\n",
      "                      \"create a ticket\"],\n",
      "         \"responses\": [\"tarnsferring the request to your pm\"],\n",
      "         \"context_set\": \"\"\n",
      "        }\n",
      "   ]\n",
      "}\n",
      "what is deep learning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: \"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        {\"tag\": \"exit\",\n",
      "         \"patterns\": [\"thank you\", \n",
      "                      \"thanks\", \n",
      "                      \"cya\",\n",
      "                      \"see you\",\n",
      "                      \"later\", \n",
      "                      \"see you later\", \n",
      "                      \"goodbye\", \n",
      "                      \"i am leaving\", \n",
      "                      \"have a good day\",\n",
      "                      \"you helped me\",\n",
      "                      \"thanks a lot\",\n",
      "                      \"thanks a ton\",\n",
      "                      \"you are the best\",\n",
      "                      \"great help\",\n",
      "                      \"too good\",\n",
      "                      \"you are a good learning buddy\"],\n",
      "         \"responses\": [\"i hope i was able to assist you, good bye\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        {\"tag\": \"olympus\",\n",
      "         \"patterns\": [\"olympus\",\n",
      "                      \"explain me how olympus works\",\n",
      "                      \"i am not able to understand olympus\",\n",
      "                      \"olympus window not working\",\n",
      "                      \"no access to olympus\",\n",
      "                      \"unable to see link in olympus\",\n",
      "                      \"no link visible on olympus\",\n",
      "                      \"whom to contact for olympus\",\n",
      "                      \"lot of problem with olympus\",\n",
      "                      \"olypus is not a good tool\",\n",
      "                      \"lot of problems with olympus\",\n",
      "                      \"how to use olympus\",\n",
      "                      \"teach me olympus\"],\n",
      "         \"responses\": [\"link: olympus wiki\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        {\"tag\": \"sl\",\n",
      "         \"patterns\": [\"i am not able to understand svm\", \n",
      "                      \"explain me how machine learning works\",\n",
      "                      \"i am not able to understand naive bayes\",\n",
      "                      \"i am not able to understand logistic regression\",\n",
      "                      \"i am not able to understand ensemble techb=niques\",\n",
      "                      \"i am not able to understand knn\",\n",
      "                      \"i am not able to understand knn imputer\",\n",
      "                      \"i am not able to understand cross validation\",\n",
      "                      \"i am not able to understand boosting\",\n",
      "                      \"i am not able to understand random forest\",\n",
      "                      \"i am not able to understand ada boosting\",\n",
      "                      \"i am not able to understand gradient boosting\",\n",
      "                      \"machine learning\",\n",
      "                      \"ml\",\n",
      "                      \"sl\", \n",
      "                      \"supervised learning\",\n",
      "                      \"knn\",\n",
      "                      \"logistic regression\",\n",
      "                      \"regression\",\n",
      "                      \"classification\",\n",
      "                      \"naive bayes\",\n",
      "                      \"nb\",\n",
      "                      \"ensemble techniques\",\n",
      "                      \"bagging\",\n",
      "                      \"boosting\",\n",
      "                      \"ada boosting\",\n",
      "                      \"ada\",\n",
      "                      \"gradient boosting\",\n",
      "                      \"hyper parameters\"],\n",
      "         \"responses\": [\"link: machine learning wiki \"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "    \n",
      "        {\"tag\": \"nn\",\n",
      "         \"patterns\": [\"what is deep learning\", \n",
      "                      \"unable to understand deep learning\",\n",
      "                      \"explain me how deep learning works\",\n",
      "                      \"i am not able to understand deep learning\",\n",
      "                      \"not able to understand neural nets\",\n",
      "                      \"very diffult to understand neural nets\",\n",
      "                      \"unable to understand neural nets\",\n",
      "                      \"ann\", \n",
      "                      \"artificial intelligence\", \n",
      "                      \"artificial neural networks\",\n",
      "                      \"weights\",\n",
      "                      \"activation function\",\n",
      "                      \"hidden layers\",\n",
      "                      \"softmax\",\n",
      "                      \"sigmoid\",\n",
      "                      \"relu\",\n",
      "                      \"otimizer\",\n",
      "                      \"forward propagation\",\n",
      "                      \"backward propagation\",\n",
      "                      \"epochs\",\n",
      "                      \"epoch\",\n",
      "                      \"what is an epoch\",\n",
      "                      \"adam\",\n",
      "                      \"sgd\"],\n",
      "         \"responses\": [\"link: neural nets wiki\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "        \n",
      "        {\"tag\": \"bot\",\n",
      "         \"patterns\": [\"what is your name\",\n",
      "                      \"who are you\",\n",
      "                      \"name please\",\n",
      "                      \"when are your hours of opertions\", \n",
      "                      \"what are your working hours\", \n",
      "                      \"hours of operation\",\n",
      "                      \"working hours\",\n",
      "                      \"hours\"],\n",
      "         \"responses\": [\"i am your virtual learning assistant\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "    \n",
      "    {\"tag\": \"profane\",\n",
      "         \"patterns\": [\"what the hell\",\n",
      "                      \"bloody stupid bot\",\n",
      "                      \"do you think you are very smart\",\n",
      "                      \"screw you\", \n",
      "                      \"i hate you\", \n",
      "                      \"you are stupid\",\n",
      "                      \"jerk\",\n",
      "                      \"you are a joke\",\n",
      "                      \"useless piece of shit\"],\n",
      "         \"responses\": [\"please use respectful words\"],\n",
      "         \"context_set\": \"\"\n",
      "        },\n",
      "    \n",
      "        {\"tag\": \"ticket\",\n",
      "         \"patterns\": [\"my problem is not solved\", \n",
      "                      \"you did not help me\", \n",
      "                      \"not a good solution\",\n",
      "                      \"bad solution\",\n",
      "                      \"not good solution\",\n",
      "                      \"no help\",\n",
      "                      \"wasted my time\",\n",
      "                      \"useless bot\",\n",
      "                      \"create a ticket\"],\n",
      "         \"responses\": [\"tarnsferring the request to your pm\"],\n",
      "         \"context_set\": \"\"\n",
      "        }\n",
      "   ]\n",
      "}\n",
      "what is your name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: I am sorry! I don't understand you\n",
      "bye\n",
      "ROBO: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
    "while(flag==True):\n",
    "    user_response = input()\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response)!=None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \",end=\"\")\n",
    "                print(response(user_response))\n",
    "                sent_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
